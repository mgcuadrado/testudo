         ____ ___ ___ ___ _ _ __   __         __ ?
        |_   | __|  _|   | | |  \ /  |    .-./'_)  [ OK ]
          | || _|\_  \| || | | | | | |  ,/___\/    [FAIL]
    ------|_||___|___/|_|-\__|__/|__/-----U-U----------------mgc--

---- bugs --------------------------------------------------

[ ] minor bug: "print_text()" prints no line information for multiline text;
  but if it did, it would take an additional almost empty line (with just the
  line information); so i don't know what to do yet

[ ] make

      make track TESTUDOOPTS="-i testudo.crc"

  not fail; the "diff" Testudo command must read the "-i" option (and probably
  others) to restrict its source track info to the specified tests; same thing
  for "make diff_report" and possibly others

[ ] if your current test suite gives one uncaught exception and you add a case
  that gives the same uncaught exception, both have the same checksum in the
  track file, and the new uncaught exception isn't reported with "make track";
  this probably happens with other types of errors or fails that are identical
  and get the same checksum; it can be detected by not only tracking the
  checksum existence, but also the cardinality (how many elements with the same
  checksum), but when reporting it, we have to resort to compromises, since
  the different instances can't be told apart; hmm, maybe keeping the checksum
  of the previous instruction as a secondary checksum can help in telling them
  apart in most cases

---- big but important things ------------------------------

[ ] R&D: can i modify the syntax for "fixture_init(...)" from
  "fixture_init(var, arg1, arg2, ...)" to "fixture_init(var, (arg1, arg2,
  ...))"? that may remove the limitations explained in the doc; but
  unfortunately, i think it's impossible, because the need the first argument
  to piggy back to add Testudo's machinery; still, to be researched fully;
  "std::apply()" and "std::make_from_tuple()" seem to get close to what we
  need, but not exactly, because we must use the constructor initializer list
  syntax, which prevents our explicitly calling out a function or constructor

[ ] write a cheatsheet in the form of a complete Testudo example that exercises
  everything, with annotations; it should fit in an A4 paper; well this seems
  impossible, since the list of all syntax names barely fits in an A4 paper;
  instead, i can try to have separate tests, each one focused on one aspect of
  Testudo and Mock Turtle, each one fitting in an A4 paper (but several small
  ones may be fitted in a single page); rundown:

      * testrengthening, "make" targets

      * test structure ("define_top_test_node", "define_test", etc)

      * regular tests (values and checks, including scopes, with-data,
        provided, etc)

      * fixtures (including fixture definition, with-fixture, visible-fixture,
        etc)

      * Mock Turtle

      * adding Testudo support to types

[ ] write more documentation about tracks

[ ] add static asserts and probably other things to improve compile error
  messages; also, tips in the doc; like

      if the compiler complains about an unknown "testudo____TEST_my_test",
      it's because you haven't defined a test node or test with the name
      "my_test"

[ ] support outputting test report in the same run as the test track

[ ] consider running each test as a separate process, so that the whole
  procedure is more resistant to crashes

[ ] what happens to "make track_progress" when something crashes?

[ ] make track progress reports more beautiful

[ ] make Testudo independent from Unix

  [X] code an alternative to "xml_to_color" ("testudo xml_to_color")

  [X] code an alternative to "xml_to_color -s" ("testudo xml_to_color_summary")

  [ ] code an alternative to "xml_to_color -l" ("testudo xml_to_color_latex")

  [X] code an alternative to "xml_to_color -b" ("testudo_xml_to_bw")

  [X] code an alternative to "xml_to_color" option "-w" (to set the width of
    the text)

  [X] no need to code an alternative to "xml_to_color" option "-r", since it's
    about stopping the conversion between the colour codes are converted to
    ANSI codes

  [ ] code a direct colour summary output format

[ ] add options (like "just report stats", "show tests", "show failed tests",
  "show failed checks", "no color", et cetera) for "testudo run"

  current situation:

      "-f <format_name>" selects format name

      "-s <node-nade>" limits the execution to the subtree rooted at the
        specified node (supports only one subtree root)

  possible extensions:

      * at the executable level,

        [X] adding several subtree roots to execute [done by a combination of
          "-s" to specify a (single) root, and "-i" to restrict the execution
          to a list of nodes and their subtrees (several "-i" specifications
          for several such nodes)

        [X] selecting subtree roots by glob

      * at the display (XSLT) level,

        [ ] showing only fails (failed stats, steps, step-ids and step
          locations)

  possible developments:

      * integrate the "XML -> text with codes -> ANSI text" (and variants
        thereof) in the executable, so that the display-level extensions are
        controlled by options passed to the executable

      * integrate "make" invocation in the executable, with a "-j <n>" option
        to be passed to "make"

[ ] code ways to monitor the progression of test results, as features are coded
  and bugs are solved; this would require a robust way of identifying check
  steps; musings:

    * explicit step ids are great, but we can't expect the user to add those
      for all steps

    * automatically-generated steps could be checksum based, and managed
      separately for separate test functions; two distinct steps with the same
      checksum in the same test function are expected to be for identical
      steps, in different locations; they can be individually identified by
      (arithmetically) adding to the checksum the occurrence number (0 for the
      first time) in the same test function

    * a specific output (display-level) mode for monitoring outputs only test
      nodes and check step ids, and stats for each test node and step id

    * some check tests can have an unknown state, if they come after an
      unexpected exception

    * questions that can be answered by the monitoring, compared to a reference
      report:

        * what is the delta for the stats of each test node

        * what are the test ids that have changed status (new OKs, new FAILs,
          new ERRs)

        * what are the new test ids, and what are the test ids that have
          disappeared

---- high priority -----------------------------------------

[ ] Google Mock lays down expectations _before_ the function runs, and i guess
  this is so the first unmet expectation stops the test; is it better than the
  approach of "mock-turtle"?

---- easy --------------------------------------------------

[ ] new kind of check, alternative to "check()" but with the same follow-ups,
  something like "ensure()" that aborts the rest of the test if the check isn't
  successful; maybe it can throw a Testudo-specific kind of exception that is
  caught and diagnosed ("test aborted because of a failed ensure"); kinda like
  "provided()" but not just for a code section but for the rest of the test

[ ] support

    check_try(...)_catch(...) << "explanation"

  and add it to the check-try-catch example in the manual; consider moving the
  explanations about "show" and "explanations" to put them after
  check-try-catch too

[ ] code "provided_try(...)_catch(...)", or separate the "_try(...)" into
  something that can be combined with "check()" and with "provided()", so that
  it later plays well with "ensure()" too; but provided() and check() have
  changed -> ensure whether "provided (check_try(...)_catch(...)) ..." works

[ ] do we support this?:

    if (not check(a+b)_equal(c))
      print_text("a+b and c+d should be equal but aren't");

[ ] other kinds of if-like macros (like "provided()")

    [ ] ensure (check(...)_...);

        kinda like an assert: if it fails, the whole test is skipped; this can
        be done by raising a Testudo-specific exception

    [ ] if_fail (check(...)_...) { print_text("indented explanation"); }

        for explanations if a test fails (explanations if a test doesn't fail
        should be written in all cases, not guarded by an if-like macro)

[ ] consider: for progress tracking, we're saying that deleting tests that
  succeed is "bad" and deleting tests that fail is "good"; i'd say we can't be
  sure; maybe we should add a new category "can't say if it's good or bad; just
  be aware you have deleted tests that succeeded, or tests that failed"

[ ] document (and add to "testarudo.ttd") this most interesting fact: if you
  define a lambda inside a test, you only need to add "this" to its captures to
  be able to use Testudo syntax in it (see "kmsxml/kmsxml.ttd")

[ ] code a "testudo" option to testrengthen ([teˈstreŋkθən]) a project (rather
  than copying and renaming ~/local/include/Makefile.template, then editing it,
  you should be able to do something like "testudo -m -l mylib -e -myexe")

[ ] check "color_no_loc_info" and the make target that use it; there's
  something fishy going on

[ ] remove "no_loc_info" in favour of "testudo xml_to_color -n" and a new
  option to "testudo run" that doesn't add location info to the XML (for "make
  save_report")

[ ] code an easy way to add new formats that differ only in construction
  options (see "testudo_format_color_text.cpp:TestFormatColorTextWithLines")

[ ] code a templated random generator system, to be used with
  "generate_data()"; generation of random data for a given type should probably
  become another aspect of "adding Testudo support for your types"

---- in progress -------------------------------------------

[ ] document

---- tasks -------------------------------------------------

[ ] should i make the exception catched by "check_try(...)_catch(...)"
  available for additional testing? yes; probably as part of a "provide
  (check_try(...)_catch(...))" structure, where the catched exception is
  available for the code in the block following the "provide(...)"

[ ] fails in not visible fixtures add to the count, but in a mysterious manner;
  maybe fails in a not visible fixture should always be made visible anyway

[ ] print results to a different stream than "cout", so that it doesn't get
  mixed with other, non-test output?

[ ] diffs can't have line number info because that would mess up big time with
  diffs (line numbers can be shifted by the addition of a single instruction),
  but at the same time, it would be good if we could spot quickly the origin of
  a difference; can this be done?

[ ] it may be possible to have a command that computes result diffs with
  respect to any past version, using the appropriate version control command

[ ] ideas to consider for the diff engine:

    [ ] find a cool name

    [ ] maybe work directly on the XML format, instead of requiring a specific
      "track" format; this may require a "flattening" of the XML report, prior
      to computing the diff

    [ ] add hyperlinks to the diff report, that take you to the specific
      location in the report (not in the source code; the _report_ should have
      hyperlinks that take you to the source code) (see "add hyperlinks to the
      report" below)

    [ ] allow the user to specify "min_length"

[ ] add hyperlinks to the report (only in XML? how to do that? maybe a specific
  viewer that shows you the report without the links, but allows you to select
  them and open an editor at the exact location, exactly like links in Emacs'
  "*compilation*" or "*grep*" buffers)

[ ] MockTurtle: when checking logs, add a way to skip logged calls until a call
  to a given method; additionally, add a way to combine the skip with the check
  (the resulting instruction would mean "check that the next call to method
  'foo' verifies 'blablablah'")

[ ] MockTurtle: add a method to filter a log so we check only calls to a set of
  methods

[ ] solve the comment in "TestFormatColorText::output_location()"

[ ] in certain contexts (such as when showing expression values of failed
  checks), multiline values may be shown that break the line-breaking
  algorithm; they show different with XML and colour-text formats (i think the
  XML format isn't fooled, whereas the colour-text format is)

[ ] check how "with_data()"-loops behave in the face of unexpected exceptions,
  and if they behave badly (to be expected), correct it

[ ] more work on source line info:

    [X] adding the feature to the XML output

    [X] adding a summarised line output: if the source code for a step is in
      the same file as the test it's in, print just the line number; otherwise,
      print the whole file+line

    [ ] ways to control wether we want no lines, lines only for failed steps,
      or lines for all steps

    [ ] add line info to the summary

[ ] do something to mock static methods; i've started (shelve "attempt at
  mocking static methods"), and it seems feasible, but it may be too much work
  compared to its expected use: who uses static methods that may need to me
  mocked?; if someone does, it's easy to just delegate a static method to a
  mocked method of a mocked singleton, so you get all the benefits of logs and
  ledgers; maybe it would be better to explain this in the doc, with an example

[ ] use the "DuneBuggy" as an example (buggy) module to write a guide

    Mayers Manx'                       [ OK ]
             -_  .--\o_,\_.     __  _,--.
    DuneBuggy____`(o)---(o)__,-' ,-'   [FAIL]

[ ] convert the LaTeX documentation into something browsable in a browser

[ ] add an XML-to-HTML filter (or two: one HTML that looks just like the text
  colour output format, and another one that can be open and closed by sections
  from a browser)

[ ] we can have several styles of "with_data()", with different report styles:

    * one that shows each value of "n", with a "[ OK ]" or "[FAIL]" tag

    * one that only shows failed values of "n" (what's already implemented)

[ ] i'm afraid nothing can be done to improve diagnostics with "make
  runsync_test" in case of a fatal error in a "with()"-loop, but check anyway

[ ] (tip for this task: check "Boost.DLL") make a main Testudo executable that
  can dynamically link any number of binary code files (".o", ".so",
  executable; dunno if all these are possible), link the Testudo node tests and
  tests found in the files, and run them, or a specified subset; this will be
  very helpful in TDD, where you want to write tests with very small overhead;
  you would compile as usual, find the place were your compile code ends up,
  and run the Testudo executable on it; some ideas:

    [X] the one that works for sure is ".so"; (done with "dlopen()")

    [ ] for ".o", maybe we could automatically just-in-time turn it into a
      ".so";

    [ ] for executables, there may be some answers (some of them
      "gcc"-specific) in https://stackoverflow.com/questions/6617679 (check all
      answers), but another solution is to have all executables that are linked
      with Testudo contain a simple server that can listen on a port for test
      execution commands (like "gimme your test tree", or "run test
      'blablablah' and send back the report to me")

  "Boost.DLL" (https://www.boost.org/doc/libs/1_74_0/doc/html/boost_dll.html)
  works with ".so", and can apparently find symbols by type, so we could have
  specific types for test nodes (including tests) (this is already the case,
  right?), and look for them; we can also have "sections" (these seem to work
  like namespaces, but i think they're Boost-specific; maybe they prepend some
  string to the symbol names, based on the section name)

[ ] make the step ids serve an additional purpose in keeping track of successes
  and failures between versions of the tested software

    [ ] add names to each individual test step? this would make test cases
      identifiable and easier to track, even in an automated way; and the names
      would serve as documentation of the test

[ ] add predetermined predicates (although this may be impossible to do
  usefully in general, since there are many possible predicates; it may be
  better to give a lot of examples for different fields of application in the
  documentation):

    is_prime() // not very useful; find better examples
    is_multiple_of(n) // ditto
    contains(s) // for strings
    matches(r) // for strings
    greater_than(x) // generic
    between(x, y) // generic

[ ] check

    [ ] Google Mock -> if appropriate, implement "mock_turtle", ha, ha, ha!

    [ ] SUnit

[ ] add a "_relapprox()", similar to "_approx()", but using the relative
  distance (absolute distance divided by greater magnitude, or by reference
  value) rather than the absolute distance; in this case, what to do about near
  zero reference values?  is the user responsible for knowing when it's near
  zero?  or should we have a mixed approach, where the check is considered
  successful if "relapprox()" is below the tolerance _or_ "approx()" is below a
  different tolerance?  we could have "_reltol()" and "_tol()", as well as
  "rel_approx_epsilon" and "approx_epsilon" (although, at this point, it may be
  advisable to clear up the possible confusion between these usages of
  "epsilon" and its usage in "numeric_limits<T>::epsilon"; should i rename
  Testudo's "epsilon" to "delta"? probly

[ ] what happens for narrow output with enough indentation?  probly an infinite
  loop; check and correct

---- low priority, some time maybe -------------------------

[ ] test with other compilers

[ ] adapt to other SOs and their compilers (i should remove the dependency on
  "bash", "make", et cetera)

[ ] lower as much as possible the C++ version requirement (i'm compiling in
  C++17 right now; can i have everything with C++14, or even with C++11?); for
  starters, "named_create.h" uses "std::any", which appeared in C++17; then,
  inline variables also appeared in C++17; and it's the same for "if
  constexpr"; so, maybe it could be done with sufficient work, but i'm not
  going to do it right now

---- done --------------------------------------------------

[X] add fixtures à la Goggle Mock? it should be done with inheritance: each
    test is not a function but a method of a class that inherits from the
    fixture; the fixture's attributes are available in the test method; they're
    initialised by the fixture constructor; the fixture can have other methods
    that can be called by the tests

[X] make "show_scope" into a proper, method-full (in "TestFormat") structure;
  the XML version should put everything that happens while in the scope inside
  an XML element

[X] what about having, instead of

      check_approx(a, b);

  the parse-safe

      check(a)_approx(b);

  or, with an explicit tolerance,

      check(a)_approx(b)_tol(1e-3);

  this would apply to "check_equal()" too; this could be done by translating
  "check(...)" to "Check(__VA_ARGS__)", and "_approx(...)", "_equal(...)", and
  "_tol(...)" to "<< Approx(__VA_ARGS__)" and so on, and having the class
  "Check" perform the check _on destruction_

[X] transfer as a new project to "mgcuadrado.com"

[X] add a try-catch around each test instruction, so that unexpected exceptions
  are caught and reported; in fact, it can't be done around each instruction,
  because it would be impossible for declarations; it must be done around each
  test

[X] "fake_declare()" is missing

[X] another idea for managing the root of the test tree: a module in
  "oxys/mathclass", for instance, would declare a "root" (that may not be a
  true root) called "oxys", and a child node to it named "mathclass"; if
  someone later declares the same root "oxys", that's ok; same thing for
  "oxys.mathclass"; the only rule is that a given node (be it a true root, a
  non-true root, or any node) can only be given a priority at most once;
  otherwise, that's an error; same thing for the title; so a possible approach
  would be for all modules in "mathclass" to declare both "oxys" and
  "oxys.mathclass" without giving them any priority, and then the module
  "mathclass/root_test" would declare "oxys.mathclass" with a priority if
  needed; if there's only one declared root, then that's a true root, and the
  test results show it as such; if there are several declared roots, they are
  all made children to a true, undeclared root

[X] actually write and test "uc.tst"

[X] document "check()_verify()" and predicates

[X] add to checks support for predicates:

    check(v.x)_verify([](auto x) { x>50.; });

[X] add logical operations with predicates:

    check(n)_verify(not is_prime and not is_multiple_of(3));

[X] add shortcuts for predicates:

    #define predicate(arg_nam, ...) \
      Predicate([](auto arg_name) { return __VA_ARGS__; })

  or even, forcing the name of the parameter

    #define predicate(...) \
      Predicate([](auto arg) { return __VA_ARGS__; })

[X] add optional names to individual test steps; like

    step("an empty map has zero size");
    check(m.size())_equal(0);

  these test step names live within the full name of the test; should i expect
  them to be C++ valid variable names, like

    step(an_empty_map_has_zero_size);

  or should i convert the spaces to underscores in the supplied string?  what
  about apostrophes?  what about periods?

[X] document "step-id" syntax

[X] simplify try-check report:

    & expression -> " exception.what() "   [ OK ]

  or

    & expression -> <no catch> ----------- [FAIL]

[X] make the separator ("print_break()") as wide as the line width

[X] correct cartouched titles that are too long for one line

[X] refactor "xml_to_color" to do all length-related computations before the
  specific terminal or LaTeX colour codes are added; this will simplify a lot
  the processing

[X] bug: the summary report doesn't show "[ERR-]" tags

[X] give "check_try_catch()" the possibility to specify the specific kind of
  exception (people may be defining exceptions not derived from
  "std::exception")

[X] (implemented a simplification; added a new task to do the rest of the work)
  maybe add "show_fixture()", that would work like "with_fixture()", but the
  latter would only mention the fixture, whereas the former would also show the
  steps in the fixture constructor and destructor (the current implementation
  of "with_fixture()" works like the intended implementation of
  "show_fixture()"; the effect could be achieved by passing the test functions
  an additional argument: the test format for the fixture; this test format
  would be the regular one for "show_fixture()" and a null one for
  "with_fixture()")

[X] change visible/hidden fixture test format handling so that even methods
  provided by the fixture and called from the test honour the visible/hidden
  spec; this can be done by having the full/null test format object be the
  "test_format" for the fixture, and the full test format be a new
  "test_format" for the test class that derives from the fixture (this
  "test_format" hides the other one); right one, the simplified implementation
  just changes the value of the only "test_format" (in the fixture) between
  null and full depending on the visibility and whether it's the test that's
  running or the fixture's constructor or destructor

[X] check unexpected exceptions in hidden fixtures still report something
  useful

[X] document "with_fixture", "visible_fixture", "fixture_member", and
  "fixture_init"

[X] in "ttest.cpp" (and all analogous files in OXYS directories), the call
  "test_root->test(..., to.subtree)" should be replaced with
  "test_root->test(..., to)", so that the test node and the option parsing can
  be extended together without having to change their clients

[X] do something so fixture attributes automatically show their existence and
  their initialisation (dunno if that's possible somehow)

[X] is there any risk anyone will ever use the sequence "||~bold~||" in a test?
  yes; so, do something about it

[X] add a new syntax "with(n, l)" that translates into something like "for (n:
  l)", so that the following instruction or "{"-"}" enclosed scope, is repeated
  for "n" spanning each value in "l"; the intention is that the repeated part
  (the following instruction or scope) should be a check using the value of
  "n", and the result of the "with()" structure should not go into details of
  each particular check, but report only a tally, and the failed ones if there
  were any; or maybe just the number of fails and a "[FAIL]" tag, without going
  into further details; ideally, the repeated part should be reported just
  once, so that the test is completely specified in the report; tricks that i'm
  testing or considering:

    * we can replace "test_format" with a special version that doesn't log
      anything, or chooses to log only when fails happen; same thing for
      "test_stats" to intercept fails; the following trick has worked for
      replacing "test_format" with a "NullTestFormatForFixtures":

        #define with(n, ...) \\\
          if (auto test_format_copy=test_format; true)
            if (auto test_format=
                  std::make_shared<testudo::NullTestFormatForFixtures>(
                    test_format_copy);
                true)
              for (auto n: __VA_ARGS__)

    * the replacement "test_format" could be a special one that reports actions
      and checks only the first time; this would have the side-effect of not
      reporting anything when "l" is empty; this may be achievable by something
      like

        template <typename T>
        struct FirstTimeWrapper {
          bool first_time;
          T t;
        };

        // the following turns a list of "T" into a list of wrapped "T", where
        // the first element has a true "first_time", and the rest have it
        // false
        template <typename T> // extend for all containers, and for generators
        list<FirstTimeWrapper<T>> first_time_wrap(list<T>);

        #define with(n, ...) \\\
        for (auto n ## _wrap: first_time_wrap(__VA_ARGS__))
          if (auto n=n ## _wrap.t; true)
            ... // use the "if (...; true)" trick above to replace
                // "test_format" with a special one that either relays or
                // doesn't relay actions and checks (wich a special syntax and
                // indentation) to the real "test_format", depending on the
                // value of "n.first_time", and reports failed values in case
                // of fails (the failed value can be passed to the special
                // "test_format" on construction, so that on any fail, it
                // prints it with a special format)

[X] do the whole "with()"-loop thing and the indentations for the XML output

[X] generalise "with()"-loop implementation into "testudo.h"

[X] clean "with()"-loop (especially "testudo_format.h")

[X] correct cartouched titles that are too long for one line in the
  "color_text" format (this already works for the conversion from "XML" format
  to coloured text) (this is making results of "make diff_xml_text_test_bw"
  hard to interpret)

[X] add capabilities

    [X] checking of theorems with prescribed or pseudorandom data series (done
      with "with()"-loops)

    [X] these are useful, because you often want to check, for instance, that a
      constructed value isn't equal to the default one; maybe there's a general
      way to negate an existing "check()" macro? (ponder: use "check_not()"
      instead of "check()"; or at least an easy way to code "_false()",
      "_not_equal()", and "_not_approx()" in a general way):

        [X] add (and use) a "_false()" macro?

        [X] add (and use) a "_not_equal()" macro?

        [X] add (and use) a "_not_approx()" macro?

[-] make "with()"-loop iteration variable "auto const &", so that its address
  can be used, and mention it in the doc (not done: it's hard to make it so it
  works with rvalues and with lvalues; dropped)

[X] in the test sequence

    with (x, numbers_2) {
      check(x)_verify(is_even);
      with (y, numbers_2)
        check(x%2)_equal(y%2);
    }

  make the indentation of only-x checks right (see "testarudo.tdd" and the
  report)

[X] correct the indentations in chained "with()"-loops; in

    with (x, numbers_1) {
      with (y, numbers_1)
        check(x%2)_equal(y%2);
      check(0)_verify(is_even);
      with (y, numbers_2)
        check(x%2)_equal(y%2);
      check(0)_verify(is_even);
      with (y, numbers_1)
        check(x%2)_equal(y%2);
    }

  we need to keep track of the indentation, and save all outputs to the
  outermost loop, together with their depth, so that on "recursively_last_time"
  each one is output at the appropriate moment (like, "ok, guys, we're now at
  depth 3; who's got sumn to output?", then "now at depth 2; outputs?"); it may
  be easy to detect outer "WithLoopLog"s, since they always have the same name;
  we may have a dummy, outer one with depth 0, and then build on this, sending
  outputs upwards when the depth is greater than 1

[X] add a count of successes for "all successful" (this can be done easily by
  saving the current "TestStats" before the "with()"-loop, then, at the end of
  the "with()"-loop, finding the difference between the current value and the
  saved one)

[X] add many predicates and some macros to help with mock object testing checks

[X] for mocks, add the capability to check the order in which different methods
  were called; can we also check, in the same go, their arguments and/or return
  values? i guess we can keep a mock-class-instance-wide log of methods called
  (by their names), and then we can reference it to the place where the
  arguments and return value are stored in the procedure-specific log:

      instance ref        method-1         method-2
     --------------    -------------    -------------
      method-1  1      "one"   "unu"      1     "un"
      method-1  2      "two"   "du"
      method-2  1      "three" "tri"
      method-1  3
     --------------    -------------    -------------

  here, the first table keeps a list of "name of method" vs "index in
  method-specific count", so we can see that the second method call was to
  "method-1", and it was the second call to "method-1", so its argument was
  "two" and its return value was "du"; similarly, the third method call was to
  "method-1", and it was the first call to "method-2", so its argument was 1
  and its return value was "un"

[X] for mocks, add the capability to check method invocation order even across
  multiple mocks (checking not only the order in which methods are called, but
  also the order in which the mocks are called on)

[X] allow functions in mock return value schedules, and use them to test
  scheduled exceptions

[X] with mock scheduled exception throwing, make it so the argument values and
  the call are logged before the exception is thrown, so that we can check it
  later

[X] code and document styled names for "testudo__TEST_PARAMETERS" and
  "testudo__TEST_ARGUMENTS" (see example in "mock_turtle_soup.ttd"), for
  declaring functions that are called from tests [this was already done, but i
  had forgotten]; a better alternative may be to get the parameters together
  into a single struct "testudo_args_t", and use "testudo_args_t" instead of
  "testudo__TEST_PARAMETERS", and "testudo_args" instead of
  "testudo__TEST_ARGUMENTS" [this one i did, but i called it "test_management"
  rather than "testudo_args"]

[X] rename "with()" to "with_data()"?

[X] document the passing of "test_management" to test-aware functions

[X] style mock macros

[X] the logic for the negated checks is wrong; i have to add a new class to
  check "_not_equal()", for instance, because it's not the same as the negation
  of "_equal()"; a clear example is checking for equalness with a
  "call_record<>" obtained with "get_call()": if we require it to be equal to
  something, it's fine that "operator==()" returns false if a "call_record<>"
  is invalid, but then, if we require it to be not-equal to something, then if
  it's invalid the check will be successful (with the current implementation);
  we want to be able to code "operator==()" and "operator not_eq()" separately,
  so that they both return false when a "call_record<>" is invalid, and then,
  "_not_equal()" will use "operator not_eq()" rather than "not operator==()",
  and everything will be fine; it may be good to add an indirection, so that
  "_equal()" calls an overloaded function "check_equal()", which by default
  calls "operator==()", and "_not_equal()" calls an overloaded function
  "check_not_equal()", which by default calls "operator not_eq()"; another,
  maybe complementary approach, is that the "check" macro returns whatever its
  second part (e.g. "_equal()") says, but ands it with "is_valid(arg)", where
  "arg" is the argument to "check()"; "is_valid(arg)" is "arg.is_valid()" if it
  is a valid expression, or true otherwise [i went with the second alternative:
  defining a separate "operator not_eq()" wasn't an alternative for "_approx()"
  or "_verify()"]

[X] turn

    { show_scope("blablablah");
      ...
    }

  into

    show_scope("blablablah") {
      ...
    }

[X] make report text from "show_scope()" terser? think also of
  "with_declare()", whose report text is much too verbose

[X] make "with_declare()" report text terser?

[X] do something so we can do "it.get_call()" and "it.next()" in one go

[X] define "abs_diff()" for "call_record<>"

[X] test "abs_diff()" for "call_record<>"

[X] document "with-declare" macro

[X] document mock objects

[X] remove the space after the version number in the document title

[X] add a "with_declare()" macro so the following works

    with_declare (auto x=get_x()) { // 'x' is only valid in this new scope
      check(x)_verify(...);
      check(x)_not_equal(...);
    }
    with_declare (auto x=get_x())
      check(x)_approx(...); // a different 'x' variable

  if we need several declarations for one scope,

    with_declare (auto [a, b]=tuple(get_a(), get_b())) {
      ...
    }

[X] protect all names (classes, methods and attributes, generated names,
  namespaces...) with "testudo___", to avoid name clashes

[X] in the doc, remove "testudo::" and explain at the beginning that all
  non-macro names are in the "testudo" namespace

[X] finish unfinished section "How to schedule mock-related behaviour"

[X] document "void_v"

[X] remove "with-data-define-top-test" and "with-data-define-test"

[X] simplify "with_data(x, list{1, 2, 3})" to "with_data(x, {1, 2, 3})"?

[X] write section "Testudo support for STL types"

[X] add validity support for STL containers (a container is valid iff all its
  elements are)

[-] not done, because there are too many options (like skipping all methods
  named "blablablah" for a given object, or irrespective of the object, or
  skipping all methods on the same object...)

  maybe code "it.skip_to(my_method)", equivalent to

    while (it.method_name() not_eq "my_method") it.next()

  maybe also "it.skip_all(my_method)", equivalent to

    while (it.method_name()=="my_method") it.next()

[X] write section "Adding Testudo support for your types"

    [X] intro

    [X] section "Validity"

    [X] section "Textual representation"

    [X] section "Equality"

    [X] section "Difference between two values"

[X] describe how to use random-generated data sets for with-data loops (look
  for "random_generator" in OXYS)

[X] write "Editor configuration"

[X] write "Using your own test definition and test instruction names"

[X] shoud i rename "is_valid()" to "is_valid_testudo()", "absdiff()" to
  "absdiff_testudo()", and so on, to avoid name clashes in client code?
  probly; the full rename scheme would be

      is_valid                 -> is_valid_testudo
                               -> testudo::is_valid
      absdiff                  -> absdiff_testudo
                               -> testudo::absdiff
      to_text                  -> to_text_testudo
      testudo::to_text_testudo -> testudo::to_text

  and possibly

                               -> are_equal_testudo
                               -> testudo::are_equal

  where "testudo::..." functions are called for general types (also when
  defining your specific version based on definitions for types you contain),
  and "..._testudo" functions are what you define in your namespace or in the
  "testudo" namespace (at your discretion); if you need the function for a
  specific type rather than a general type, you can use "..._testudo" if you
  know it exists (and thus avoid including "testudo_base.h"); the way
  "testudo::..." finds "..._testudo" names for all types is

      * for basic types (including STL), they're defined in the same "testudo"
        namespace

      * for user types, by argument-dependent lookup
        (https://en.cppreference.com/w/cpp/language/adl)

  don't forget to update the documentation; at the same time, i can remove the
  possibility to have "is_valid()" as a method, since it isn't necessary; the
  sequence of lookups would be here (alternatives are listed in order of
  consideration):

      testudo::is_valid -----> type_namespace::is_valid_testudo
                          `--> testudo::is_valid_testudo -----> true

      testudo::absdiff -----> type_namespace::absdiff_testudo
                         `--> testudo::absdiff_testudo

      testudo::to_text -----> type_namespace::to_text_testudo
                         `--> testudo::to_text_testudo -----> os << t
                                                         `--> "<...>"

      testudo::are_equal -----> type_namespace::are_equal_testudo
                           `--> testudo::are_equal -----> t==u

[X] add the possibility to use a type's "to_text()" method for textual
  representation?

[X] document Mock Turtle's "is_always()", "is_never()", "is_constant()", and
  "all_different()"

[X] add source line information to test step reports?  musings:

    * the test title cartouche can include the file and line number as this (in
      the same colour as the line):
         ________________________________________________
        | {testudo.testarudo.blablablah} blablablah test |
        `-----testarudo.ttd:40---------------------------'

    * failed checks can include the file (if not the same as that of the latest
      title) and line number as this (in the same colour as the line):

        % 2*1+1 ~ is_even : 9 -------------------------47- [FAIL]

[X] rename "show_scope()" to "in_scope()"?

[X] make last argument of "define_test_node()" and similar macros optional?  or
  rather, remove the need for an identifier, and use the title as the key

[X] document the new syntax for names and titles for tests and test nodes

[X] if possible, enable and document structured binding syntax for
  "with_data()"

[X] add a new type of check similar to "_true", but allowing you to specify
  what expression values to print if it fails; for instance:

    check(a+b==b+a)_show(a, b)_true;
    check(x<f(y))_show(x, y)_true

  this would be just "[ OK ]" if ok, but print the value of "tuple{a, b}" if
  not

[X] code "with_data_multiline()", where data items are separated by newlines;
  use the report for "testudo.diff.longest common subsequence" as an example to
  improve; i think this may require some parsing of the text representation for
  the data

[X] add installation procedure; should install the binaries to "~/local/bin/"
  and the headers to "~/local/include/testudo/"

[X] document installation procedure

[X] make installation procedure configurable

[X] "with_data changed" entries in a track report ("make diff_dune_buggy_test"
  for instance) are reported with wrong line number: the line number should be
  that of the outermost "with-data" loop beginning; instead, it's that of an
  inner step (probably the last failed one, but i'm not sure)

[X] support result tracking directly from the makefile: the latest results must
  be stored with a given name (e.g., "testudo.results"), and there must be a
  make targed that computes and shows the diff; there must be another target
  (kinda like the current "success_test") that stores the current results; this
  must always be done just prior to committing a new version

[X] document result tracking with "make save_track" and "make track_progress"

[X] in "testudo_format_track.cpp", "crc(code, content)" should be used rather
  than "crc(content)"; this will cause a format break for existing tracks

[X] update README.md with the latest developments (especially progress
  tracking)

[X] when there's no progress, "make track_progress" should print something like
  "no progress and no regress" (in blue?)

[X] add a black-and-white version of "make track_progress" where regresses
  stand out (with colour, it's easy, but in black-and-white, i'll have to find
  some way to make them obvious)

  [X] make track progress report clearer

[X] "make -j 8 diff_track" warns about "jobserver unavailable: using -j1"

[X] code "check_multiline()" for (string) arguments that span several lines; or
  make "check()" (and maybe others) detect automatically multiline arguments,
  and display them accordingly; did the automatic version

[X] document: in section "The test code", the #include's use quotes, but they
  should use square brackets

[X] document: make target "show_report_bw" isn't listed

[X] document the ".tth" convention for testudo-enhancing headers

[X] shouldn't '\' chars at the end of broken lines appear blue?

[X] should the result of a "_true_for()" check depend on the validity of the
  arguments to "_true_for()"?

[X] deprecate explicitly #include-ing "testudo_ext.h"; it should be always
  included by "testudo.h", since it is very small (209 lines) (well, yes, but
  it brings in a few STL headers that aren't tiny) compared to what's already
  included when #include-ing "testudo.h" (1745 lines); change doc accordingly

[X] code "given()" to check preconditions before undertaking certain test
  steps:

      given(a)_not_equal(0)
      given(b>0)_true {
        show_value(b/a);
        check(a*(b/a))_equal(b);
      }

  if a "given()" fails, the tests it guards aren't even run, and an error is
  tallied (as if an unexpected exception had been thrown); "given()" could
  actually be implemented as a special "check()" that throws on fail, but it
  should be enclosed in a try-catch, so that it doesn't obliterate the rest of
  the test; may not be trivial

  other names for "given()":

      * "assuming()"

      * "provided()"

[X] support "iomanip" on the report output?  like, sometimes, i'd like to print
  an int in hexadecimal, so i'd like to be able to do something like

    show_value(setw(4), setfill('0'), hex, integer_value);

  how about adding this feature to the fail report of checks, for instance?

  current situation:

      macros use "testudo::to_text(value)"

      "testudo::to_text(value)" uses "to_text_testudo(value)"

      if no user-provided "to_text_testudo(value)" exists, this reverts to
        "testudo::to_text_testudo(value)"

      "testudo::to_text_testudo(value)" uses "os << value"

      if no user-provided "os << value" exists, this reverts to standard

  what we want:

      macros use "testudo::to_stream(vfos, value)" ("vfos" stands for "value
        formatter ostream"; it's a configurable macro, same as "show_value" and
        all the others; the state of "vfos" is saved for equality checks, for
        instance, since both the reference and the test value must be formatted
        with the same format, and some manipulators [the ones with an argument,
        like "setw(...)"] are cleared when used; so we must save the state,
        output the first value, recover the state, and output the second value)

      "testudo::to_stream(vfos, value)" uses "to_stream_testudo(vfos, value)"
        (but saves the state of "vfos" before calling it and restores it
        afterwards: we don't want "to_stream_testudo(vfos, value)" to affect
        "vfos" in unpredictable ways; but! one-time manipulators should be
        cleared as expected; maybe do some 'vfos << ""' that we don't use)

      if no user-provided "to_stream_testudo(vfos, value)" exists, this reverts
        to "testudo::to_stream_testudo(vfos, value)"

      "testudo::to_stream_testudo(vfos, value)" uses "os << value"

      if no user-provided "os << value" exists, this reverts to standard

[X] make "show_value()" &al actually work in "with_data()"?  why would i put
  such a thing in a "with_data()" otherwise?

[-] is there any way to code a templated version of "with_data()", so that the
  parameters (types or otherwise) can be used as a template argument in the
  "with_data()" body?  i doubt it: "a template declaration cannot appear at
  block scope"; if i try the following trick:

  void function() {
    struct TemplateHolder {
      template <typename T>
      void test_things(T const &t) { ... }
    };
  }

  i get "invalid declaration of member template in local class";
  "https://en.cppreference.com/w/cpp/language/class" states "local classes
  other than closure types cannot have member templates"; it may be achievable
  with lambdas with auto arguments, but then we'd be templatising only on
  types; the only sensible approach seems to be "test-aware functions" (see
  thusly-named section in manual)

[X] allow the function assigned to a mock method to depend on the mocked
  method's arguments (see "How to schedule mock-method behaviour" in the
  manual)

[X] maybe implement multiple inheritance in mock classes, by giving
  "MockClass<>" any number of arguments?

[X] i believe in the following

    with_data(a, generate_data(100, random_int))
      with_data(b, generate_data(100, random_int)) {
        show(a);
        show(b);
      }

  the values "b" takes are different for each value of "a" (a fresh list of
  random values for "b" is created for each "a" value); we may want the
  opposite behaviour: having just 100 values for "b" that we cross with the 100
  "a" values; this could be achieved with something like

    with_data((a, b), cartesian(generate_data(100, random_int),
                                generate_data(100, random_int))) {
      show(a);
      show(b);
    }

  but consider this: for repeatability, we might want to set the seed before
  every usage of "generate_data()"; if so, the following would achieve almost
  the desired effect (although the "cartesian()" thing would be cleaner):

    perform(set_seed(1234));
    with_data(a, generate_data(100, random_int)) {
      perform(set_seed(8888);
      with_data(b, generate_data(100, random_int)) {
        show(a);
        show(b);
      }
    }

[X] in track reports, distinguish between [bad] ([OK] instances turning [FAIL])
  and [very-bad] (having new [ERR-] instances)

[X] i think "make diff_test" should make both "make diff_report" and "make
  diff_track", even if the former returns an error (i.e., if there are diffs in
  the report); no, wait, that's "make diff_track"; what does "make diff_test"
  do?

[X] harmonise Makefile targets that report results

[X] make the XML parser resistant to unfinished XML, so that we can have at
  least an incomplete report even if the test crashes

[X] line number for "with fixture" in the report seems to be wrong

[X] when i add a test that adds an uncaught exception in a new with-data loop,
  the track is reported as "no progress and no regress"; it should be very bad
  instead

[X] consider removing predicate checks ("VERIFY", "PREDICATE", et cetera) in
  favour of "TRUE_FOR"

[X] "literature"

    [I] write a reimagining of the turtle (actually tortoise...) and the hare,
      where the hare isn't actually lazy but unfocused, and runs with all his
      speed but in any direction, not stopping in time to check his course, and
      the turtle walks more slowly, but always checking his direction; thus,
      the turtle always moves towards the goal, where the hare doesn't most of
      the time, and actually often runs away from it; is coding with tests
      slower than coding without them? only in terms of total lines written per
      hour (most of which are wrong for the hare-style coding); if you count
      functionality per hour, it's way faster, and the result is cleaner, more
      elegant, better documented (tests document too), easier to understand,
      and more maintainable

    [X] then, maybe write about Testarudo's friend, Mock Turtle, connecting the
      software to Lewis Carroll's character

[X] is "make runsync_test" the same as "make runsync_test_with_lines"?

[X] add the possibility for fixtures to have construct arguments; these should
  be passed with something like "with_fixture_arguments(...)" when defining a
  test with a fixture

[X] extend "fixture_init()" so that the attribute it initialises can have any
  number of arguments (right now, it only works with exactly one)

[X] remove support for the "tol" syntax

[X] remove "Check<...>::set()" and "Check<...>::get()" in favour of keeping the
  necessary test-specific info in the "CheckTest"-derived class; this is
  currently only used by "CheckApprox" for old-style tolerance setting ("tol"
  and "stol")

[-] turn "_true_for(...)" into "_true _for(...)" so that we can combine
  "_for()" with all other check types, like in

      check(a[i])_equal(b)_for(i);

  no: see rationale about "_for()" in "development.txt"

[X] ensure it's easy to transform "check(a+b)_approx(c)" into something like
  "check(approx(a+b, c))_true_for(a, b, c)" so that we can add a value-based
  explanation when the test fails

[X] show additional relevant expressions but not their values even when a test
  succeeds? because they contribute to the test with their validity; otherwise,
  two tests apparently identical may have different outcomes, just because one
  of them is also testing for the validity of a variable, without it showing in
  any way

[X] new kind of follow-up to "check()": "_explain_fail(explanation)" to print
  an explanation if a check fails; maybe with the Googletest syntax:

      check(a[i])_equal(b)
        << "a[i] didn't match b for index " << i
        << "which it should because blablah";

  (but don't forget "_true_for" and "_false_for")

  ideas for macro names:

      * for relevant values, "_for", "_how", "_what", "_values", "_show",
        "_see", "_from"

      * for fail explanation, "_explain_fail", "_why", "_because", "_reason",
        "_explain"

  conversation avec Migouche: "_show()", "_explain()"

[X] should relevant values ("_show(...)") and explanations ("<< ...") be on
  their own indented line after the failed test text and label? that would
  probably be clearer, and failing is the exceptional case; related to that,
  right now, when a "with_data()" datum fails, the datum is shown _before_ the
  failed test; maybe we shouldd unify with the modified relevant values and
  explanations, and show the datum _after_ the failed test

[X] extend "show_value()" to accept any number of arguments

[X] for failed "not-" checks ("not-equal", "not-approx") add "nay" after the
  "?" to show clearly what we're checking for

[X] add coloured text results to code snippet examples in the guide

[X] [...] in addition to that, should we support this, à la googletest?:

    check(a+b)_equal(c)
      << "the sum of a (" << a << ") and b (" << b << ") should be c";

  although this would be better written

    check(a+b)_equal(c)_with_values(a, b)
      << "the sum of a and b should be c";

  or even

    check(a+b)_equal(c)_with_values(a, b)
      _with_message(""the sum of a and b should be c");

